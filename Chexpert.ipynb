{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T08:13:33.475012Z",
     "iopub.status.busy": "2025-08-03T08:13:33.474676Z",
     "iopub.status.idle": "2025-08-03T16:54:37.140297Z",
     "shell.execute_reply": "2025-08-03T16:54:37.139245Z",
     "shell.execute_reply.started": "2025-08-03T08:13:33.474989Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MEMORY-SAFE ENSEMBLE MODEL SYSTEM\n",
      "Fixed Label Handling with Binary Classification\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "- USE_ALL_TRAIN: True\n",
      "- MEMORY_SAFE_MODE: True\n",
      "- MODEL_TYPE: ensemble\n",
      "- MAX_SAMPLES_PER_CHUNK: 50000\n",
      "- BATCH_SIZE: 32\n",
      "- EPOCHS: 16\n",
      "Using device: cuda\n",
      "GPU: Tesla T4\n",
      "GPU Memory - Allocated: 0.00GB, Reserved: 0.00GB\n",
      "Loading and preparing data...\n",
      "Total train samples: 223414\n",
      "Total test samples: 234\n",
      "Number of unique patients: 64540\n",
      "\n",
      "Data split:\n",
      "Train: 190358 samples\n",
      "Val: 33056 samples\n",
      "Test: 234 samples\n",
      "\n",
      "USE_ALL_TRAIN=True detected. Using memory-safe data handling...\n",
      "Total training samples after combining: 223414\n",
      "New split - Train: 201072, Val: 22342\n",
      "\n",
      "Initializing Memory-Safe ENSEMBLE + LR System...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
      "100%|██████████| 30.8M/30.8M [00:00<00:00, 222MB/s]\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 260MB/s]\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-3dd342df.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-3dd342df.pth\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 30.7MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.16GB, Reserved: 0.17GB\n",
      "\n",
      "Training ENSEMBLE Feature Extractor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/16 [Train]: 100%|██████████| 6283/6283 [43:35<00:00,  2.40it/s, loss=0.0602]\n",
      "Validation: 100%|██████████| 699/699 [04:53<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=0.0636, Train AUC=0.6356, Val Loss=0.0602, Val AUC=0.7239\n",
      "Saved best model with Val AUC: 0.7239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/16 [Train]: 100%|██████████| 6283/6283 [25:41<00:00,  4.08it/s, loss=0.0529]\n",
      "Validation: 100%|██████████| 699/699 [02:15<00:00,  5.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss=0.0580, Train AUC=0.7102, Val Loss=0.0578, Val AUC=0.7509\n",
      "Saved best model with Val AUC: 0.7509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/16 [Train]: 100%|██████████| 6283/6283 [25:45<00:00,  4.07it/s, loss=0.0579]\n",
      "Validation: 100%|██████████| 699/699 [02:27<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss=0.0563, Train AUC=0.7365, Val Loss=0.0569, Val AUC=0.7639\n",
      "Saved best model with Val AUC: 0.7639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/16 [Train]: 100%|██████████| 6283/6283 [26:50<00:00,  3.90it/s, loss=0.0549]\n",
      "Validation: 100%|██████████| 699/699 [02:21<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss=0.0553, Train AUC=0.7518, Val Loss=0.0562, Val AUC=0.7699\n",
      "Saved best model with Val AUC: 0.7699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/16 [Train]: 100%|██████████| 6283/6283 [26:52<00:00,  3.90it/s, loss=0.0511]\n",
      "Validation: 100%|██████████| 699/699 [02:25<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss=0.0547, Train AUC=0.7611, Val Loss=0.0559, Val AUC=0.7744\n",
      "Saved best model with Val AUC: 0.7744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/16 [Train]: 100%|██████████| 6283/6283 [26:33<00:00,  3.94it/s, loss=0.0593]\n",
      "Validation: 100%|██████████| 699/699 [02:21<00:00,  4.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss=0.0542, Train AUC=0.7688, Val Loss=0.0557, Val AUC=0.7780\n",
      "Saved best model with Val AUC: 0.7780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/16 [Train]: 100%|██████████| 6283/6283 [32:58<00:00,  3.18it/s, loss=0.0563]  \n",
      "Validation: 100%|██████████| 699/699 [02:23<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss=0.0538, Train AUC=0.7747, Val Loss=0.0555, Val AUC=0.7809\n",
      "Saved best model with Val AUC: 0.7809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/16 [Train]: 100%|██████████| 6283/6283 [26:24<00:00,  3.96it/s, loss=0.0541]\n",
      "Validation: 100%|██████████| 699/699 [02:19<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss=0.0534, Train AUC=0.7797, Val Loss=0.0554, Val AUC=0.7832\n",
      "Saved best model with Val AUC: 0.7832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/16 [Train]: 100%|██████████| 6283/6283 [25:40<00:00,  4.08it/s, loss=0.0452]\n",
      "Validation: 100%|██████████| 699/699 [02:24<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss=0.0531, Train AUC=0.7844, Val Loss=0.0553, Val AUC=0.7851\n",
      "Saved best model with Val AUC: 0.7851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/16 [Train]: 100%|██████████| 6283/6283 [27:03<00:00,  3.87it/s, loss=0.0572]\n",
      "Validation: 100%|██████████| 699/699 [02:20<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss=0.0528, Train AUC=0.7873, Val Loss=0.0552, Val AUC=0.7856\n",
      "Saved best model with Val AUC: 0.7856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/16 [Train]: 100%|██████████| 6283/6283 [26:40<00:00,  3.93it/s, loss=0.0473]\n",
      "Validation: 100%|██████████| 699/699 [02:29<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Train Loss=0.0526, Train AUC=0.7904, Val Loss=0.0551, Val AUC=0.7862\n",
      "Saved best model with Val AUC: 0.7862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/16 [Train]: 100%|██████████| 6283/6283 [26:06<00:00,  4.01it/s, loss=0.0524]\n",
      "Validation: 100%|██████████| 699/699 [02:16<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Train Loss=0.0524, Train AUC=0.7930, Val Loss=0.0550, Val AUC=0.7868\n",
      "Saved best model with Val AUC: 0.7868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/16 [Train]: 100%|██████████| 6283/6283 [26:18<00:00,  3.98it/s, loss=0.05]  \n",
      "Validation: 100%|██████████| 699/699 [02:20<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Train Loss=0.0523, Train AUC=0.7954, Val Loss=0.0551, Val AUC=0.7869\n",
      "Saved best model with Val AUC: 0.7869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/16 [Train]: 100%|██████████| 6283/6283 [26:18<00:00,  3.98it/s, loss=0.0556]\n",
      "Validation: 100%|██████████| 699/699 [02:22<00:00,  4.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Train Loss=0.0522, Train AUC=0.7960, Val Loss=0.0551, Val AUC=0.7878\n",
      "Saved best model with Val AUC: 0.7878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/16 [Train]: 100%|██████████| 6283/6283 [26:22<00:00,  3.97it/s, loss=0.0525]\n",
      "Validation: 100%|██████████| 699/699 [02:28<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Train Loss=0.0521, Train AUC=0.7968, Val Loss=0.0550, Val AUC=0.7880\n",
      "Saved best model with Val AUC: 0.7880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/16 [Train]: 100%|██████████| 6283/6283 [26:05<00:00,  4.01it/s, loss=0.0522]\n",
      "Validation: 100%|██████████| 699/699 [02:19<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Train Loss=0.0521, Train AUC=0.7972, Val Loss=0.0551, Val AUC=0.7878\n",
      "Loaded best model with Val AUC: 0.7880\n",
      "GPU Memory - Allocated: 0.32GB, Reserved: 0.49GB\n",
      "\n",
      "Extracting features...\n",
      "\n",
      "Extracting train features in memory-safe mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:   0%|          | 1/6283 [00:03<6:40:10,  3.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:   2%|▏         | 101/6283 [00:29<30:10,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:   3%|▎         | 201/6283 [00:54<30:07,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:   5%|▍         | 301/6283 [01:20<29:01,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:   6%|▋         | 401/6283 [01:45<28:47,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:   8%|▊         | 501/6283 [02:11<28:17,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  10%|▉         | 601/6283 [02:36<28:10,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  11%|█         | 701/6283 [03:02<27:26,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  13%|█▎        | 801/6283 [03:28<27:29,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  14%|█▍        | 901/6283 [03:53<26:29,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  16%|█▌        | 1001/6283 [04:19<25:53,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  18%|█▊        | 1101/6283 [04:44<25:35,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  19%|█▉        | 1201/6283 [05:10<24:49,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  21%|██        | 1301/6283 [05:35<24:26,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  22%|██▏       | 1401/6283 [06:01<23:49,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  24%|██▍       | 1501/6283 [06:26<23:33,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  25%|██▍       | 1563/6283 [06:53<4:38:49,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 0 with 50016 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  25%|██▌       | 1601/6283 [07:03<22:58,  3.40it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  27%|██▋       | 1701/6283 [07:28<22:59,  3.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  29%|██▊       | 1801/6283 [07:54<21:59,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  30%|███       | 1901/6283 [08:19<21:19,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  32%|███▏      | 2001/6283 [08:45<21:14,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  33%|███▎      | 2101/6283 [09:11<20:36,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  35%|███▌      | 2201/6283 [09:36<20:15,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  37%|███▋      | 2301/6283 [10:02<19:29,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  38%|███▊      | 2401/6283 [10:27<19:14,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  40%|███▉      | 2501/6283 [10:53<18:26,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  41%|████▏     | 2601/6283 [11:18<18:25,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  43%|████▎     | 2701/6283 [11:44<17:39,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  45%|████▍     | 2801/6283 [12:09<17:12,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  46%|████▌     | 2901/6283 [12:35<16:45,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  48%|████▊     | 3001/6283 [13:00<16:10,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  49%|████▉     | 3101/6283 [13:26<15:36,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  50%|████▉     | 3126/6283 [13:43<3:05:15,  3.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 1 with 50016 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  51%|█████     | 3201/6283 [14:02<15:09,  3.39it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  53%|█████▎    | 3301/6283 [14:28<14:50,  3.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  54%|█████▍    | 3401/6283 [14:54<14:01,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  56%|█████▌    | 3501/6283 [15:19<13:45,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  57%|█████▋    | 3601/6283 [15:45<13:11,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  59%|█████▉    | 3701/6283 [16:10<12:54,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  60%|██████    | 3801/6283 [16:36<12:08,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  62%|██████▏   | 3901/6283 [17:01<12:01,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  64%|██████▎   | 4001/6283 [17:27<11:13,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  65%|██████▌   | 4101/6283 [17:52<10:47,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  67%|██████▋   | 4201/6283 [18:18<10:11,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  68%|██████▊   | 4301/6283 [18:44<09:55,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  70%|███████   | 4401/6283 [19:09<09:16,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  72%|███████▏  | 4501/6283 [19:35<08:42,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  73%|███████▎  | 4601/6283 [20:00<08:19,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  75%|███████▍  | 4689/6283 [20:33<1:34:06,  3.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 2 with 50016 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  75%|███████▍  | 4701/6283 [20:37<09:02,  2.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  76%|███████▋  | 4801/6283 [21:02<07:31,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  78%|███████▊  | 4901/6283 [21:28<06:48,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  80%|███████▉  | 5001/6283 [21:54<06:14,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  81%|████████  | 5101/6283 [22:19<05:45,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  83%|████████▎ | 5201/6283 [22:44<05:20,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  84%|████████▍ | 5301/6283 [23:10<04:47,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  86%|████████▌ | 5401/6283 [23:35<04:19,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  88%|████████▊ | 5501/6283 [24:01<03:50,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  89%|████████▉ | 5601/6283 [24:26<03:19,  3.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  91%|█████████ | 5701/6283 [24:52<02:51,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  92%|█████████▏| 5801/6283 [25:17<02:22,  3.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  94%|█████████▍| 5901/6283 [25:43<01:55,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  96%|█████████▌| 6001/6283 [26:09<01:23,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  97%|█████████▋| 6101/6283 [26:34<00:54,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features:  99%|█████████▊| 6201/6283 [27:00<00:24,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features: 100%|█████████▉| 6252/6283 [27:24<01:48,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved chunk 3 with 50016 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting train features: 100%|██████████| 6283/6283 [27:32<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final chunk 4 with 992 samples\n",
      "\n",
      "Extracting validation features in memory-safe mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features:   0%|          | 1/699 [00:00<11:28,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features:  14%|█▍        | 101/699 [00:26<03:01,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features:  29%|██▉       | 201/699 [00:52<02:24,  3.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features:  43%|████▎     | 301/699 [01:17<01:57,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features:  57%|█████▋    | 401/699 [01:43<01:28,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features:  72%|███████▏  | 501/699 [02:08<00:57,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features:  86%|████████▌ | 601/699 [02:34<00:29,  3.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting validation features: 100%|██████████| 699/699 [02:59<00:00,  3.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final chunk 0 with 22342 samples\n",
      "\n",
      "Extracting test features in memory-safe mode...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting test features:  12%|█▎        | 1/8 [00:01<00:11,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory - Allocated: 0.34GB, Reserved: 0.49GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting test features: 100%|██████████| 8/8 [00:05<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final chunk 0 with 234 samples\n",
      "Deleted feature extractor to free memory\n",
      "GPU Memory - Allocated: 0.02GB, Reserved: 0.04GB\n",
      "\n",
      "Training Logistic Regression Classifiers from chunks...\n",
      "Fitting StandardScaler on all chunks...\n",
      "\n",
      "Training LR for No Finding...\n",
      "No Finding - Pos samples: 22379/223398 (10.0%), Weights: pos=4.99, neg=0.56\n",
      "No Finding - Training completed\n",
      "\n",
      "Training LR for Enlarged Cardiomediastinum...\n",
      "Enlarged Cardiomediastinum - Pos samples: 10797/223398 (4.8%), Weights: pos=10.35, neg=0.53\n",
      "Enlarged Cardiomediastinum - Training completed\n",
      "\n",
      "Training LR for Cardiomegaly...\n",
      "Cardiomegaly - Pos samples: 26997/223398 (12.1%), Weights: pos=4.14, neg=0.57\n",
      "Cardiomegaly - Training completed\n",
      "\n",
      "Training LR for Lung Opacity...\n",
      "Lung Opacity - Pos samples: 105573/223398 (47.3%), Weights: pos=1.06, neg=0.95\n",
      "Lung Opacity - Training completed\n",
      "\n",
      "Training LR for Lung Lesion...\n",
      "Lung Lesion - Pos samples: 9186/223398 (4.1%), Weights: pos=12.16, neg=0.52\n",
      "Lung Lesion - Training completed\n",
      "\n",
      "Training LR for Edema...\n",
      "Edema - Pos samples: 65226/223398 (29.2%), Weights: pos=1.71, neg=0.71\n",
      "Edema - Training completed\n",
      "\n",
      "Training LR for Consolidation...\n",
      "Consolidation - Pos samples: 14783/223398 (6.6%), Weights: pos=7.56, neg=0.54\n",
      "Consolidation - Training completed\n",
      "\n",
      "Training LR for Pneumonia...\n",
      "Pneumonia - Pos samples: 6039/223398 (2.7%), Weights: pos=18.50, neg=0.51\n",
      "Pneumonia - Training completed\n",
      "\n",
      "Training LR for Atelectasis...\n",
      "Atelectasis - Pos samples: 67112/223398 (30.0%), Weights: pos=1.66, neg=0.71\n",
      "Atelectasis - Training completed\n",
      "\n",
      "Training LR for Pneumothorax...\n",
      "Pneumothorax - Pos samples: 19447/223398 (8.7%), Weights: pos=5.74, neg=0.55\n",
      "Pneumothorax - Training completed\n",
      "\n",
      "Training LR for Pleural Effusion...\n",
      "Pleural Effusion - Pos samples: 97807/223398 (43.8%), Weights: pos=1.14, neg=0.89\n",
      "Pleural Effusion - Training completed\n",
      "\n",
      "Training LR for Pleural Other...\n",
      "Pleural Other - Pos samples: 3523/223398 (1.6%), Weights: pos=31.71, neg=0.51\n",
      "Pleural Other - Training completed\n",
      "\n",
      "Training LR for Fracture...\n",
      "Fracture - Pos samples: 9039/223398 (4.0%), Weights: pos=12.36, neg=0.52\n",
      "Fracture - Training completed\n",
      "\n",
      "Training LR for Support Devices...\n",
      "Support Devices - Pos samples: 115989/223398 (51.9%), Weights: pos=0.96, neg=1.04\n",
      "Support Devices - Training completed\n",
      "\n",
      "Making predictions from chunks...\n",
      "Cleaned up 7 temporary chunk files\n",
      "\n",
      "================================================================================\n",
      "EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Per-Disease Performance:\n",
      "--------------------------------------------------------------------------------\n",
      "Disease                             AUC       AP       F1     Sens     Spec\n",
      "--------------------------------------------------------------------------------\n",
      "No Finding                        0.843    0.383    0.538    0.658    0.847\n",
      "Enlarged Cardiomediastinum        0.475    0.500    0.635    0.982    0.032\n",
      "Cardiomegaly                      0.786    0.670    0.602    0.588    0.849\n",
      "Lung Opacity                      0.904    0.919    0.846    0.849    0.815\n",
      "Lung Lesion                       0.455    0.008    0.000    0.000    0.455\n",
      "Edema                             0.917    0.736    0.739    0.756    0.931\n",
      "Consolidation                     0.881    0.563    0.531    0.515    0.930\n",
      "Pneumonia                         0.575    0.056    0.000    0.000    0.973\n",
      "Atelectasis                       0.835    0.697    0.729    0.825    0.773\n",
      "Pneumothorax                      0.756    0.085    0.154    0.375    0.876\n",
      "Pleural Effusion                  0.922    0.853    0.741    0.642    0.964\n",
      "Pleural Other                     0.828    0.024    0.000    0.000    0.828\n",
      "Support Devices                   0.899    0.863    0.825    0.860    0.811\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Overall Performance Summary:\n",
      "--------------------------------------------------------------------------------\n",
      "Mean AUC: 0.775 ± 0.159\n",
      "Mean AP: 0.489 ± 0.329\n",
      "Mean F1: 0.488 ± 0.315\n",
      "Mean Sensitivity: 0.542 ± 0.334\n",
      "Mean Specificity: 0.776 ± 0.249\n",
      "\n",
      "Results saved to 'ensemble_lr_memory_safe_results.csv'\n",
      "\n",
      "================================================================================\n",
      "CLASSIFICATION COMPLETED\n",
      "================================================================================\n",
      "Model Type: ENSEMBLE\n",
      "Feature Dimension: 1024\n",
      "Classifier: Logistic Regression\n",
      "Memory Safe Mode: True\n",
      "USE_ALL_TRAIN: True\n",
      "Test Samples: 234\n",
      "\n",
      "Top 5 Diseases by AUC:\n",
      "Pleural Effusion: AUC=0.922, F1=0.741\n",
      "Edema: AUC=0.917, F1=0.739\n",
      "Lung Opacity: AUC=0.904, F1=0.846\n",
      "Support Devices: AUC=0.899, F1=0.825\n",
      "Consolidation: AUC=0.881, F1=0.531\n",
      "GPU Memory - Allocated: 0.02GB, Reserved: 0.04GB\n",
      "\n",
      "Total time: 520.9 minutes\n",
      "\n",
      "================================================================================\n",
      "Training completed successfully!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, roc_curve, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================== CONFIGURATION ==================\n",
    "DISEASE_LABELS = [\n",
    "    'No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', \n",
    "    'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', \n",
    "    'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion',\n",
    "    'Pleural Other', 'Fracture', 'Support Devices'\n",
    "]\n",
    "\n",
    "# Optimized hyperparameters\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "GRADIENT_ACCUMULATION = 2\n",
    "EPOCHS = 16\n",
    "LEARNING_RATE = 1e-4\n",
    "TRAIN_SPLIT = 0.85\n",
    "USE_ALL_TRAIN = True\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "# Model selection\n",
    "USE_ENSEMBLE = True\n",
    "MODEL_TYPE = 'ensemble'\n",
    "\n",
    "# Memory optimization flags\n",
    "MEMORY_SAFE_MODE = True\n",
    "CLEAR_CACHE_FREQ = 20\n",
    "ENABLE_CHECKPOINTING = True\n",
    "MAX_SAMPLES_PER_CHUNK = 50000\n",
    "FEATURE_SAVE_CHUNKS = True\n",
    "DELETE_INTERMEDIATE = True\n",
    "\n",
    "# ================== MEMORY UTILITIES ==================\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressive memory clearing\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def print_memory_stats():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "\n",
    "# ================== ASYMMETRIC LOSS (FIXED) ==================\n",
    "\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    \"\"\"Asymmetric Loss for imbalanced multi-label classification\"\"\"\n",
    "    def __init__(self, gamma_neg=4, gamma_pos=1, clip=0.05, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # Probabilities\n",
    "        x_sigmoid = torch.sigmoid(x)\n",
    "        xs_pos = x_sigmoid\n",
    "        xs_neg = 1 - x_sigmoid\n",
    "        \n",
    "        # Asymmetric Clipping\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            xs_neg = (xs_neg + self.clip).clamp(max=1)\n",
    "        \n",
    "        # Basic CE calculation\n",
    "        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n",
    "        los_neg = (1 - y) * torch.log(xs_neg.clamp(min=self.eps))\n",
    "        \n",
    "        # Asymmetric Focusing\n",
    "        if self.gamma_neg > 0 or self.gamma_pos > 0:\n",
    "            pt0 = xs_pos * y\n",
    "            pt1 = xs_neg * (1 - y)\n",
    "            pt = pt0 + pt1\n",
    "            one_sided_gamma = self.gamma_pos * y + self.gamma_neg * (1 - y)\n",
    "            one_sided_w = torch.pow(1 - pt, one_sided_gamma)\n",
    "            los_pos *= one_sided_w\n",
    "            los_neg *= one_sided_w\n",
    "        \n",
    "        loss = -los_pos - los_neg\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "# ================== FEATURE EXTRACTORS (UNCHANGED) ==================\n",
    "\n",
    "class DenseNetExtractor(nn.Module):\n",
    "    \"\"\"DenseNet-121 feature extractor\"\"\"\n",
    "    def __init__(self, num_features=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.densenet = models.densenet121(pretrained=True)\n",
    "        num_ftrs = self.densenet.classifier.in_features\n",
    "        self.densenet.classifier = nn.Identity()\n",
    "        \n",
    "        # Feature projection\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, num_features),\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.densenet(x)\n",
    "        features = self.projection(features)\n",
    "        return features\n",
    "\n",
    "class ResNetExtractor(nn.Module):\n",
    "    \"\"\"ResNet-50 feature extractor\"\"\"\n",
    "    def __init__(self, num_features=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.resnet = models.resnet50(pretrained=True)\n",
    "        num_ftrs = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Identity()\n",
    "        \n",
    "        # Feature projection\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, num_features),\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.resnet(x)\n",
    "        features = self.projection(features)\n",
    "        return features\n",
    "\n",
    "class EfficientNetExtractor(nn.Module):\n",
    "    \"\"\"EfficientNet-B0 feature extractor\"\"\"\n",
    "    def __init__(self, num_features=512):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.efficientnet = models.efficientnet_b0(pretrained=True)\n",
    "        num_ftrs = self.efficientnet.classifier[1].in_features\n",
    "        self.efficientnet.classifier = nn.Identity()\n",
    "        \n",
    "        # Feature projection\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, num_features),\n",
    "            nn.BatchNorm1d(num_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.efficientnet(x)\n",
    "        features = self.projection(features)\n",
    "        return features\n",
    "\n",
    "# ================== ENSEMBLE FEATURE EXTRACTOR ==================\n",
    "\n",
    "class EnsembleFeatureExtractor(nn.Module):\n",
    "    \"\"\"Ensemble of DenseNet, ResNet, and EfficientNet\"\"\"\n",
    "    def __init__(self, num_features=1024, fusion_method='concat'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fusion_method = fusion_method\n",
    "        \n",
    "        # Initialize individual extractors\n",
    "        self.densenet = DenseNetExtractor(num_features=512)\n",
    "        self.resnet = ResNetExtractor(num_features=512)\n",
    "        self.efficientnet = EfficientNetExtractor(num_features=512)\n",
    "        \n",
    "        # Freeze early layers to save memory\n",
    "        self._freeze_early_layers()\n",
    "        \n",
    "        # Feature fusion layer\n",
    "        if fusion_method == 'concat':\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(512 * 3, 2048),\n",
    "                nn.BatchNorm1d(2048),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.4),\n",
    "                nn.Linear(2048, num_features),\n",
    "                nn.BatchNorm1d(num_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.3)\n",
    "            )\n",
    "        elif fusion_method == 'attention':\n",
    "            self.attention_weights = nn.Parameter(torch.ones(3) / 3)\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(512, num_features),\n",
    "                nn.BatchNorm1d(num_features),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(0.3)\n",
    "            )\n",
    "        \n",
    "        self.num_features = num_features\n",
    "    \n",
    "    def _freeze_early_layers(self):\n",
    "        \"\"\"Freeze early layers to reduce memory usage\"\"\"\n",
    "        # Freeze DenseNet early blocks\n",
    "        for name, param in self.densenet.densenet.features.named_parameters():\n",
    "            if 'denseblock3' not in name and 'denseblock4' not in name:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Freeze ResNet early layers\n",
    "        for name, param in self.resnet.resnet.named_parameters():\n",
    "            if 'layer3' not in name and 'layer4' not in name:\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Freeze EfficientNet early layers\n",
    "        for name, param in self.efficientnet.efficientnet.features.named_parameters():\n",
    "            if not any(x in name for x in ['6', '7']):\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract features from each model\n",
    "        densenet_feat = self.densenet(x)\n",
    "        resnet_feat = self.resnet(x)\n",
    "        efficientnet_feat = self.efficientnet(x)\n",
    "        \n",
    "        if self.fusion_method == 'concat':\n",
    "            # Concatenate features\n",
    "            combined = torch.cat([densenet_feat, resnet_feat, efficientnet_feat], dim=1)\n",
    "            output = self.fusion(combined)\n",
    "        elif self.fusion_method == 'attention':\n",
    "            # Weighted average with learned attention\n",
    "            weights = F.softmax(self.attention_weights, dim=0)\n",
    "            combined = weights[0] * densenet_feat + weights[1] * resnet_feat + weights[2] * efficientnet_feat\n",
    "            output = self.fusion(combined)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# ================== DATASET WITH FIXED LABELS ==================\n",
    "\n",
    "class CheXpertDataset(Dataset):\n",
    "    \"\"\"Dataset with preprocessing - FIXED LABEL HANDLING\"\"\"\n",
    "    def __init__(self, dataframe, image_dir, transform=None, training=True):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.training = training\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Build path\n",
    "        path = row['Path']\n",
    "        if 'CheXpert-v1.0-small' in path:\n",
    "            relative_path = path.replace('CheXpert-v1.0-small/', '')\n",
    "        else:\n",
    "            relative_path = path\n",
    "        \n",
    "        full_path = os.path.join(self.image_dir, relative_path)\n",
    "        \n",
    "        # Read image\n",
    "        image = None\n",
    "        if os.path.exists(full_path):\n",
    "            try:\n",
    "                image = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)\n",
    "                \n",
    "                # CLAHE preprocessing\n",
    "                if image is not None:\n",
    "                    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "                    image = clahe.apply(image)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # Create placeholder if needed\n",
    "        if image is None:\n",
    "            image = np.ones((IMAGE_SIZE, IMAGE_SIZE), dtype=np.uint8) * 128\n",
    "        \n",
    "        # Resize\n",
    "        if image.shape[0] != IMAGE_SIZE or image.shape[1] != IMAGE_SIZE:\n",
    "            image = cv2.resize(image, (IMAGE_SIZE, IMAGE_SIZE), interpolation=cv2.INTER_LANCZOS4)\n",
    "        \n",
    "        # Convert to RGB\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "        \n",
    "        # Get labels - FIXED: Return binary labels for classification\n",
    "        labels = []\n",
    "        for disease in DISEASE_LABELS:\n",
    "            if disease in row:\n",
    "                label = row[disease]\n",
    "                if pd.isna(label):\n",
    "                    label = 0.0\n",
    "                elif label == -1:\n",
    "                    # Handle uncertain labels\n",
    "                    if self.training:\n",
    "                        if disease in ['Atelectasis', 'Edema', 'Pleural Effusion']:\n",
    "                            label = 1.0  # Positive\n",
    "                        else:\n",
    "                            label = 0.0  # Negative\n",
    "                    else:\n",
    "                        label = 0.0\n",
    "                else:\n",
    "                    label = float(label)\n",
    "                    \n",
    "                # Apply label smoothing ONLY during neural network training\n",
    "                if self.training and LABEL_SMOOTHING > 0 and label in [0.0, 1.0]:\n",
    "                    if label == 1.0:\n",
    "                        label = 1.0 - LABEL_SMOOTHING/2\n",
    "                    else:\n",
    "                        label = LABEL_SMOOTHING/2\n",
    "                        \n",
    "                labels.append(label)\n",
    "            else:\n",
    "                labels.append(0.0)\n",
    "        \n",
    "        labels = np.array(labels, dtype=np.float32)\n",
    "        \n",
    "        return image, labels, idx\n",
    "\n",
    "# ================== MEMORY-SAFE TRAINING SYSTEM ==================\n",
    "\n",
    "class MemorySafeEnsembleLRSystem:\n",
    "    \"\"\"Memory-optimized Ensemble + LR System\"\"\"\n",
    "    def __init__(self, device='cuda', num_features=1024, model_type='ensemble'):\n",
    "        self.device = device\n",
    "        self.num_features = num_features\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        # Initialize feature extractor\n",
    "        if model_type == 'ensemble':\n",
    "            self.feature_extractor = EnsembleFeatureExtractor(\n",
    "                num_features=num_features, \n",
    "                fusion_method='concat'\n",
    "            ).to(device)\n",
    "        elif model_type == 'densenet':\n",
    "            self.feature_extractor = DenseNetExtractor(num_features=num_features).to(device)\n",
    "        elif model_type == 'resnet':\n",
    "            self.feature_extractor = ResNetExtractor(num_features=num_features).to(device)\n",
    "        elif model_type == 'efficientnet':\n",
    "            self.feature_extractor = EfficientNetExtractor(num_features=num_features).to(device)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        self.scaler = StandardScaler()\n",
    "        self.classifiers = {}\n",
    "    \n",
    "    def extract_features_memory_safe(self, dataloader, phase='train', save_to_disk=True):\n",
    "        \"\"\"Extract features with memory optimization\"\"\"\n",
    "        self.feature_extractor.eval()\n",
    "        \n",
    "        if save_to_disk and MEMORY_SAFE_MODE:\n",
    "            return self._extract_features_to_chunks(dataloader, phase)\n",
    "        else:\n",
    "            return self._extract_features_standard(dataloader, phase)\n",
    "    \n",
    "    def _extract_features_to_chunks(self, dataloader, phase):\n",
    "        \"\"\"Extract features and save in chunks to avoid memory overflow\"\"\"\n",
    "        print(f\"\\nExtracting {phase} features in memory-safe mode...\")\n",
    "        \n",
    "        chunk_idx = 0\n",
    "        chunk_features = []\n",
    "        chunk_labels = []\n",
    "        saved_chunks = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, labels, _) in enumerate(tqdm(dataloader, desc=f'Extracting {phase} features')):\n",
    "                images = images.to(self.device)\n",
    "                \n",
    "                # Extract features\n",
    "                features = self.feature_extractor(images)\n",
    "                \n",
    "                # Move to CPU immediately and delete GPU tensor\n",
    "                features_cpu = features.cpu().numpy()\n",
    "                del features\n",
    "                \n",
    "                chunk_features.append(features_cpu)\n",
    "                chunk_labels.append(labels.numpy())\n",
    "                \n",
    "                # Clear GPU cache frequently\n",
    "                if batch_idx % CLEAR_CACHE_FREQ == 0:\n",
    "                    clear_memory()\n",
    "                    if batch_idx % 100 == 0:\n",
    "                        print_memory_stats()\n",
    "                \n",
    "                # Save chunk when it reaches max size\n",
    "                current_samples = sum(f.shape[0] for f in chunk_features)\n",
    "                if current_samples >= MAX_SAMPLES_PER_CHUNK:\n",
    "                    # Stack and save chunk\n",
    "                    chunk_feat_array = np.vstack(chunk_features)\n",
    "                    chunk_label_array = np.vstack(chunk_labels)\n",
    "                    \n",
    "                    chunk_file = f'{phase}_features_chunk_{chunk_idx}.npz'\n",
    "                    np.savez_compressed(chunk_file, \n",
    "                                      features=chunk_feat_array, \n",
    "                                      labels=chunk_label_array)\n",
    "                    saved_chunks.append(chunk_file)\n",
    "                    \n",
    "                    print(f\"Saved chunk {chunk_idx} with {chunk_feat_array.shape[0]} samples\")\n",
    "                    \n",
    "                    # Clear chunk data\n",
    "                    chunk_features = []\n",
    "                    chunk_labels = []\n",
    "                    chunk_idx += 1\n",
    "                    \n",
    "                    # Force memory cleanup\n",
    "                    del chunk_feat_array, chunk_label_array\n",
    "                    clear_memory()\n",
    "        \n",
    "        # Save remaining data\n",
    "        if chunk_features:\n",
    "            chunk_feat_array = np.vstack(chunk_features)\n",
    "            chunk_label_array = np.vstack(chunk_labels)\n",
    "            \n",
    "            chunk_file = f'{phase}_features_chunk_{chunk_idx}.npz'\n",
    "            np.savez_compressed(chunk_file, \n",
    "                              features=chunk_feat_array, \n",
    "                              labels=chunk_label_array)\n",
    "            saved_chunks.append(chunk_file)\n",
    "            \n",
    "            print(f\"Saved final chunk {chunk_idx} with {chunk_feat_array.shape[0]} samples\")\n",
    "        \n",
    "        return saved_chunks\n",
    "    \n",
    "    def _extract_features_standard(self, dataloader, phase):\n",
    "        \"\"\"Standard feature extraction (original method)\"\"\"\n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, labels, _) in enumerate(tqdm(dataloader, desc=f'Extracting {phase} features')):\n",
    "                images = images.to(self.device)\n",
    "                \n",
    "                # Extract features\n",
    "                features = self.feature_extractor(images)\n",
    "                \n",
    "                features_list.append(features.cpu().numpy())\n",
    "                labels_list.append(labels.numpy())\n",
    "                \n",
    "                # Clear cache periodically\n",
    "                if batch_idx % 30 == 0:\n",
    "                    gc.collect()\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "        \n",
    "        features = np.vstack(features_list)\n",
    "        labels = np.vstack(labels_list)\n",
    "        \n",
    "        return features, labels\n",
    "    \n",
    "    def train_lr_classifiers_from_chunks(self, train_chunks, val_chunks=None):\n",
    "        \"\"\"Train LR classifiers from saved chunks - FIXED WITH MANUAL CLASS WEIGHTS\"\"\"\n",
    "        print(\"\\nTraining Logistic Regression Classifiers from chunks...\")\n",
    "        \n",
    "        # First, fit StandardScaler on all data\n",
    "        print(\"Fitting StandardScaler on all chunks...\")\n",
    "        for chunk_file in train_chunks:\n",
    "            data = np.load(chunk_file)\n",
    "            features = data['features']\n",
    "            self.scaler.partial_fit(features)\n",
    "            del data, features\n",
    "            clear_memory()\n",
    "        \n",
    "        # Train classifier for each disease\n",
    "        for disease_idx, disease in enumerate(DISEASE_LABELS):\n",
    "            print(f'\\nTraining LR for {disease}...')\n",
    "            \n",
    "            # First pass: collect all labels to compute class weights\n",
    "            all_y = []\n",
    "            for chunk_file in train_chunks:\n",
    "                data = np.load(chunk_file)\n",
    "                labels = data['labels'][:, disease_idx]\n",
    "                labels_binary = (labels > 0.5).astype(int)\n",
    "                all_y.extend(labels_binary)\n",
    "                del data, labels\n",
    "            \n",
    "            all_y = np.array(all_y)\n",
    "            \n",
    "            if all_y.sum() < 10:\n",
    "                print(f'Too few positive samples for {disease}, skipping...')\n",
    "                continue\n",
    "            \n",
    "            # Calculate class weights manually\n",
    "            n_samples = len(all_y)\n",
    "            n_pos = all_y.sum()\n",
    "            n_neg = n_samples - n_pos\n",
    "            \n",
    "            # Balanced class weights\n",
    "            pos_weight = n_samples / (2.0 * n_pos) if n_pos > 0 else 1.0\n",
    "            neg_weight = n_samples / (2.0 * n_neg) if n_neg > 0 else 1.0\n",
    "            \n",
    "            print(f\"{disease} - Pos samples: {n_pos}/{n_samples} ({n_pos/n_samples*100:.1f}%), \"\n",
    "                  f\"Weights: pos={pos_weight:.2f}, neg={neg_weight:.2f}\")\n",
    "            \n",
    "            try:\n",
    "                # Use SGDClassifier without class_weight parameter\n",
    "                from sklearn.linear_model import SGDClassifier\n",
    "                \n",
    "                lr = SGDClassifier(\n",
    "                    loss='log',\n",
    "                    penalty='l2',\n",
    "                    alpha=0.01,\n",
    "                    max_iter=1000,\n",
    "                    random_state=42,\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                # Train on chunks with manual sample weights\n",
    "                for epoch in range(2):  # Multiple passes over data\n",
    "                    for chunk_file in train_chunks:\n",
    "                        data = np.load(chunk_file)\n",
    "                        features = data['features']\n",
    "                        labels = data['labels'][:, disease_idx]\n",
    "                        labels_binary = (labels > 0.5).astype(int)\n",
    "                        \n",
    "                        # Create sample weights based on class\n",
    "                        sample_weights = np.where(labels_binary == 1, pos_weight, neg_weight)\n",
    "                        \n",
    "                        # Scale features\n",
    "                        features_scaled = self.scaler.transform(features)\n",
    "                        \n",
    "                        # Partial fit with sample weights\n",
    "                        lr.partial_fit(features_scaled, labels_binary, \n",
    "                                     classes=[0, 1], sample_weight=sample_weights)\n",
    "                        \n",
    "                        del data, features, labels, features_scaled, sample_weights\n",
    "                        clear_memory()\n",
    "                \n",
    "                self.classifiers[disease] = lr\n",
    "                print(f'{disease} - Training completed')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training LR for {disease}: {e}\")\n",
    "    \n",
    "    def train_lr_classifiers(self, train_features, train_labels):\n",
    "        \"\"\"Original LR training method - FIXED FOR BINARY LABELS\"\"\"\n",
    "        print(\"\\nTraining Logistic Regression Classifiers...\")\n",
    "        \n",
    "        # Standardize features\n",
    "        train_features_scaled = self.scaler.fit_transform(train_features)\n",
    "        \n",
    "        for i, disease in enumerate(DISEASE_LABELS):\n",
    "            print(f'\\nTraining LR for {disease}...')\n",
    "            \n",
    "            y = train_labels[:, i]\n",
    "            # Convert to binary labels (threshold at 0.5)\n",
    "            y_binary = (y > 0.5).astype(int)\n",
    "            \n",
    "            if y_binary.sum() < 10:\n",
    "                print(f'Too few positive samples for {disease}, skipping...')\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Logistic Regression\n",
    "                lr = LogisticRegression(\n",
    "                    penalty='l2',\n",
    "                    C=1.0,\n",
    "                    max_iter=1000,\n",
    "                    class_weight='balanced',\n",
    "                    random_state=42,\n",
    "                    solver='lbfgs',\n",
    "                    n_jobs=-1\n",
    "                )\n",
    "                \n",
    "                lr.fit(train_features_scaled, y_binary)\n",
    "                \n",
    "                # Validate\n",
    "                train_pred = lr.predict_proba(train_features_scaled)[:, 1]\n",
    "                train_auc = roc_auc_score(y_binary, train_pred)\n",
    "                \n",
    "                self.classifiers[disease] = lr\n",
    "                print(f'{disease} - Train AUC: {train_auc:.3f}')\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error training LR for {disease}: {e}\")\n",
    "    \n",
    "    def predict_from_chunks(self, test_chunks):\n",
    "        \"\"\"Make predictions from saved chunks\"\"\"\n",
    "        print(\"\\nMaking predictions from chunks...\")\n",
    "        \n",
    "        all_predictions = {disease: [] for disease in DISEASE_LABELS}\n",
    "        \n",
    "        for chunk_file in test_chunks:\n",
    "            data = np.load(chunk_file)\n",
    "            features = data['features']\n",
    "            \n",
    "            # Scale features\n",
    "            features_scaled = self.scaler.transform(features)\n",
    "            \n",
    "            # Predict for each disease\n",
    "            for disease in DISEASE_LABELS:\n",
    "                if disease in self.classifiers:\n",
    "                    lr = self.classifiers[disease]\n",
    "                    pred = lr.predict_proba(features_scaled)[:, 1]\n",
    "                    all_predictions[disease].extend(pred)\n",
    "                else:\n",
    "                    all_predictions[disease].extend(np.zeros(len(features)))\n",
    "            \n",
    "            del data, features, features_scaled\n",
    "            clear_memory()\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        predictions = {disease: np.array(preds) for disease, preds in all_predictions.items()}\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def predict(self, test_features):\n",
    "        \"\"\"Original prediction method\"\"\"\n",
    "        test_features_scaled = self.scaler.transform(test_features)\n",
    "        \n",
    "        predictions = {}\n",
    "        for disease in DISEASE_LABELS:\n",
    "            if disease in self.classifiers:\n",
    "                lr = self.classifiers[disease]\n",
    "                predictions[disease] = lr.predict_proba(test_features_scaled)[:, 1]\n",
    "            else:\n",
    "                predictions[disease] = np.zeros(len(test_features))\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def train_feature_extractor(self, train_loader, val_loader, epochs=3):\n",
    "        \"\"\"Train feature extractor - FIXED WITH ASYMMETRIC LOSS\"\"\"\n",
    "        print(f\"\\nTraining {self.model_type.upper()} Feature Extractor...\")\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.num_features, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, len(DISEASE_LABELS))\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Loss function - Now defined in the code\n",
    "        criterion = AsymmetricLoss(gamma_neg=4, gamma_pos=1, clip=0.05)\n",
    "        \n",
    "        # Optimizer\n",
    "        if self.model_type == 'ensemble':\n",
    "            params = [\n",
    "                {'params': self.feature_extractor.densenet.parameters(), 'lr': LEARNING_RATE * 0.1},\n",
    "                {'params': self.feature_extractor.resnet.parameters(), 'lr': LEARNING_RATE * 0.1},\n",
    "                {'params': self.feature_extractor.efficientnet.parameters(), 'lr': LEARNING_RATE * 0.1},\n",
    "                {'params': self.feature_extractor.fusion.parameters(), 'lr': LEARNING_RATE},\n",
    "                {'params': self.classifier.parameters(), 'lr': LEARNING_RATE}\n",
    "            ]\n",
    "        else:\n",
    "            params = [\n",
    "                {'params': self.feature_extractor.parameters(), 'lr': LEARNING_RATE * 0.1},\n",
    "                {'params': self.classifier.parameters(), 'lr': LEARNING_RATE}\n",
    "            ]\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(params, weight_decay=1e-5)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        \n",
    "        # Mixed precision\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        best_val_auc = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.feature_extractor.train()\n",
    "            self.classifier.train()\n",
    "            \n",
    "            train_loss = 0\n",
    "            train_preds = []\n",
    "            train_labels = []\n",
    "            \n",
    "            pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]')\n",
    "            for batch_idx, (images, labels, _) in enumerate(pbar):\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                # Mixed precision training\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    features = self.feature_extractor(images)\n",
    "                    outputs = self.classifier(features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                loss = loss / GRADIENT_ACCUMULATION\n",
    "                \n",
    "                # Backward\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                if (batch_idx + 1) % GRADIENT_ACCUMULATION == 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        list(self.feature_extractor.parameters()) + list(self.classifier.parameters()), \n",
    "                        max_norm=1.0\n",
    "                    )\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                train_loss += loss.item() * GRADIENT_ACCUMULATION\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    train_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                    train_labels.append(labels.cpu().numpy())\n",
    "                \n",
    "                pbar.set_postfix({'loss': loss.item() * GRADIENT_ACCUMULATION})\n",
    "                \n",
    "                # Memory management\n",
    "                if MEMORY_SAFE_MODE and batch_idx % CLEAR_CACHE_FREQ == 0:\n",
    "                    clear_memory()\n",
    "            \n",
    "            # Calculate training metrics - convert to binary for AUC calculation\n",
    "            train_preds = np.vstack(train_preds)\n",
    "            train_labels = np.vstack(train_labels)\n",
    "            train_labels_binary = (train_labels > 0.5).astype(int)\n",
    "            train_auc = self.calculate_multi_label_auc(train_labels_binary, train_preds)\n",
    "            \n",
    "            # Validation\n",
    "            val_loss, val_auc = self.validate(val_loader, criterion)\n",
    "            \n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            \n",
    "            print(f'Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Train AUC={train_auc:.4f}, '\n",
    "                  f'Val Loss={val_loss:.4f}, Val AUC={val_auc:.4f}')\n",
    "            \n",
    "            # Save best model\n",
    "            if val_auc > best_val_auc:\n",
    "                best_val_auc = val_auc\n",
    "                torch.save({\n",
    "                    'feature_extractor': self.feature_extractor.state_dict(),\n",
    "                    'classifier': self.classifier.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'val_auc': val_auc\n",
    "                }, f'best_{self.model_type}_model.pth')\n",
    "                print(f'Saved best model with Val AUC: {best_val_auc:.4f}')\n",
    "            \n",
    "            scheduler.step()\n",
    "        \n",
    "        # Load best model\n",
    "        checkpoint = torch.load(f'best_{self.model_type}_model.pth')\n",
    "        self.feature_extractor.load_state_dict(checkpoint['feature_extractor'])\n",
    "        self.classifier.load_state_dict(checkpoint['classifier'])\n",
    "        print(f\"Loaded best model with Val AUC: {checkpoint['val_auc']:.4f}\")\n",
    "    \n",
    "    def validate(self, val_loader, criterion):\n",
    "        \"\"\"Validation\"\"\"\n",
    "        self.feature_extractor.eval()\n",
    "        self.classifier.eval()\n",
    "        \n",
    "        val_loss = 0\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, _ in tqdm(val_loader, desc='Validation'):\n",
    "                images = images.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                \n",
    "                with torch.cuda.amp.autocast():\n",
    "                    features = self.feature_extractor(images)\n",
    "                    outputs = self.classifier(features)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_preds.append(torch.sigmoid(outputs).cpu().numpy())\n",
    "                val_labels.append(labels.cpu().numpy())\n",
    "        \n",
    "        val_preds = np.vstack(val_preds)\n",
    "        val_labels = np.vstack(val_labels)\n",
    "        # Convert to binary for AUC calculation\n",
    "        val_labels_binary = (val_labels > 0.5).astype(int)\n",
    "        val_auc = self.calculate_multi_label_auc(val_labels_binary, val_preds)\n",
    "        \n",
    "        return val_loss / len(val_loader), val_auc\n",
    "    \n",
    "    def calculate_multi_label_auc(self, y_true, y_pred):\n",
    "        \"\"\"Calculate mean AUC\"\"\"\n",
    "        aucs = []\n",
    "        for i in range(y_true.shape[1]):\n",
    "            if y_true[:, i].sum() > 0 and y_true[:, i].sum() < len(y_true):\n",
    "                try:\n",
    "                    auc = roc_auc_score(y_true[:, i], y_pred[:, i])\n",
    "                    aucs.append(auc)\n",
    "                except:\n",
    "                    pass\n",
    "        return np.mean(aucs) if aucs else 0\n",
    "\n",
    "# ================== MAIN PIPELINE WITH MEMORY SAFETY ==================\n",
    "\n",
    "def main_memory_safe():\n",
    "    \"\"\"Main pipeline with memory optimization\"\"\"\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Paths\n",
    "    train_csv = '/kaggle/input/chexpert/train.csv'\n",
    "    valid_csv = '/kaggle/input/chexpert/valid.csv'\n",
    "    image_dir = '/kaggle/input/chexpert/'\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # Set seeds\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(42)\n",
    "    \n",
    "    # Initial memory check\n",
    "    clear_memory()\n",
    "    print_memory_stats()\n",
    "    \n",
    "    # Load data\n",
    "    train_data, val_data, test_df = prepare_data(train_csv, valid_csv, image_dir)\n",
    "    \n",
    "    # Handle USE_ALL_TRAIN flag\n",
    "    if USE_ALL_TRAIN and MEMORY_SAFE_MODE:\n",
    "        print(\"\\nUSE_ALL_TRAIN=True detected. Using memory-safe data handling...\")\n",
    "        # Combine train and validation data\n",
    "        all_train_data = pd.concat([train_data, val_data], ignore_index=True)\n",
    "        print(f\"Total training samples after combining: {len(all_train_data)}\")\n",
    "        \n",
    "        # Split into train/val for model training\n",
    "        train_size = int(0.9 * len(all_train_data))\n",
    "        train_data = all_train_data[:train_size]\n",
    "        val_data = all_train_data[train_size:]\n",
    "        \n",
    "        print(f\"New split - Train: {len(train_data)}, Val: {len(val_data)}\")\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform, val_transform = create_transforms()\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CheXpertDataset(train_data, image_dir, train_transform, training=True)\n",
    "    val_dataset = CheXpertDataset(val_data, image_dir, val_transform, training=False)\n",
    "    test_dataset = CheXpertDataset(test_df, image_dir, val_transform, training=False)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=BATCH_SIZE,\n",
    "        shuffle=True, num_workers=2,  # Reduced workers for memory\n",
    "        pin_memory=False,  # Disable pin_memory to save RAM\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=BATCH_SIZE,\n",
    "        shuffle=False, num_workers=2,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=BATCH_SIZE,\n",
    "        shuffle=False, num_workers=2,\n",
    "        pin_memory=False\n",
    "    )\n",
    "    \n",
    "    # Initialize system\n",
    "    print(f\"\\nInitializing Memory-Safe {MODEL_TYPE.upper()} + LR System...\")\n",
    "    system = MemorySafeEnsembleLRSystem(device=device, num_features=1024, model_type=MODEL_TYPE)\n",
    "    print_memory_stats()\n",
    "    \n",
    "    # Train feature extractor\n",
    "    system.train_feature_extractor(train_loader, val_loader, epochs=EPOCHS)\n",
    "    \n",
    "    # Clear memory after training\n",
    "    clear_memory()\n",
    "    print_memory_stats()\n",
    "    \n",
    "    # Extract features with memory safety\n",
    "    print(\"\\nExtracting features...\")\n",
    "    \n",
    "    if MEMORY_SAFE_MODE and USE_ALL_TRAIN:\n",
    "        # Extract features in chunks\n",
    "        train_chunks = system.extract_features_memory_safe(train_loader, 'train', save_to_disk=True)\n",
    "        val_chunks = system.extract_features_memory_safe(val_loader, 'validation', save_to_disk=True)\n",
    "        test_chunks = system.extract_features_memory_safe(test_loader, 'test', save_to_disk=True)\n",
    "        \n",
    "        # Delete feature extractor to free memory before training classifiers\n",
    "        if DELETE_INTERMEDIATE:\n",
    "            del system.feature_extractor\n",
    "            if hasattr(system, 'classifier'):\n",
    "                del system.classifier\n",
    "            clear_memory()\n",
    "            print(\"Deleted feature extractor to free memory\")\n",
    "            print_memory_stats()\n",
    "        \n",
    "        # Train LR classifiers from chunks\n",
    "        system.train_lr_classifiers_from_chunks(train_chunks + val_chunks)\n",
    "        \n",
    "        # Make predictions from chunks\n",
    "        predictions = system.predict_from_chunks(test_chunks)\n",
    "        \n",
    "        # Load test labels\n",
    "        test_labels = []\n",
    "        for chunk_file in test_chunks:\n",
    "            data = np.load(chunk_file)\n",
    "            test_labels.append(data['labels'])\n",
    "            del data\n",
    "        test_labels = np.vstack(test_labels)\n",
    "        \n",
    "        # Clean up chunk files\n",
    "        all_chunks = train_chunks + val_chunks + test_chunks\n",
    "        for chunk_file in all_chunks:\n",
    "            if os.path.exists(chunk_file):\n",
    "                os.remove(chunk_file)\n",
    "        print(f\"Cleaned up {len(all_chunks)} temporary chunk files\")\n",
    "        \n",
    "    else:\n",
    "        # Standard extraction (original method)\n",
    "        train_features, train_labels = system.extract_features_memory_safe(train_loader, 'train', save_to_disk=False)\n",
    "        val_features, val_labels = system.extract_features_memory_safe(val_loader, 'validation', save_to_disk=False)\n",
    "        test_features, test_labels = system.extract_features_memory_safe(test_loader, 'test', save_to_disk=False)\n",
    "        \n",
    "        # Combine train and validation\n",
    "        all_train_features = np.vstack([train_features, val_features])\n",
    "        all_train_labels = np.vstack([train_labels, val_labels])\n",
    "        \n",
    "        print(f\"\\nCombined training set: {len(all_train_features)} samples\")\n",
    "        \n",
    "        # Train LR classifiers\n",
    "        system.train_lr_classifiers(all_train_features, all_train_labels)\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"\\nMaking predictions...\")\n",
    "        predictions = system.predict(test_features)\n",
    "    \n",
    "    # Convert test labels to binary for evaluation\n",
    "    test_labels_binary = (test_labels > 0.5).astype(int)\n",
    "    \n",
    "    # Evaluate\n",
    "    results = evaluate_model(predictions, test_labels_binary, test_df)\n",
    "    \n",
    "    # Save results\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df.to_csv(f'{MODEL_TYPE}_lr_memory_safe_results.csv')\n",
    "    print(f\"\\nResults saved to '{MODEL_TYPE}_lr_memory_safe_results.csv'\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CLASSIFICATION COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Model Type: {MODEL_TYPE.upper()}\")\n",
    "    print(f\"Feature Dimension: {system.num_features}\")\n",
    "    print(f\"Classifier: Logistic Regression\")\n",
    "    print(f\"Memory Safe Mode: {MEMORY_SAFE_MODE}\")\n",
    "    print(f\"USE_ALL_TRAIN: {USE_ALL_TRAIN}\")\n",
    "    print(f\"Test Samples: {len(test_labels)}\")\n",
    "    \n",
    "    # Print top 5 diseases by AUC\n",
    "    print(\"\\nTop 5 Diseases by AUC:\")\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['AUC'], reverse=True)[:5]\n",
    "    for disease, metrics in sorted_results:\n",
    "        print(f\"{disease}: AUC={metrics['AUC']:.3f}, F1={metrics['F1']:.3f}\")\n",
    "    \n",
    "    # Final memory check\n",
    "    print_memory_stats()\n",
    "    \n",
    "    print(f\"\\nTotal time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    \n",
    "    return system, results\n",
    "\n",
    "# ================== REQUIRED FUNCTIONS ==================\n",
    "\n",
    "def prepare_data(train_csv, valid_csv, image_dir):\n",
    "    \"\"\"Prepare data - FIXED FOR BINARY LABELS\"\"\"\n",
    "    print(\"Loading and preparing data...\")\n",
    "    \n",
    "    train_df = pd.read_csv(train_csv)\n",
    "    test_df = pd.read_csv(valid_csv)\n",
    "    \n",
    "    print(f\"Total train samples: {len(train_df)}\")\n",
    "    print(f\"Total test samples: {len(test_df)}\")\n",
    "    \n",
    "    # Extract patient IDs\n",
    "    def extract_patient_id(path):\n",
    "        parts = path.split('/')\n",
    "        for part in parts:\n",
    "            if part.startswith('patient'):\n",
    "                return part\n",
    "        return None\n",
    "    \n",
    "    train_df['PatientID'] = train_df['Path'].apply(extract_patient_id)\n",
    "    test_df['PatientID'] = test_df['Path'].apply(extract_patient_id)\n",
    "    \n",
    "    # Remove invalid samples\n",
    "    train_df = train_df[train_df['PatientID'].notna()]\n",
    "    test_df = test_df[test_df['PatientID'].notna()]\n",
    "    \n",
    "    # Handle uncertain labels - FIXED: convert to binary\n",
    "    u_ones = ['Atelectasis', 'Edema', 'Pleural Effusion']\n",
    "    u_zeros = ['Cardiomegaly', 'Consolidation', 'Pneumonia', 'Pneumothorax']\n",
    "    \n",
    "    for df in [train_df, test_df]:\n",
    "        for disease in DISEASE_LABELS:\n",
    "            if disease in df.columns:\n",
    "                df[disease] = df[disease].fillna(0)\n",
    "                \n",
    "                if disease in u_ones:\n",
    "                    # Uncertain as positive (U-Ones policy)\n",
    "                    df[disease] = df[disease].replace(-1, 1)\n",
    "                elif disease in u_zeros:\n",
    "                    # Uncertain as negative (U-Zeros policy)  \n",
    "                    df[disease] = df[disease].replace(-1, 0)\n",
    "                else:\n",
    "                    # Uncertain as negative for other diseases\n",
    "                    df[disease] = df[disease].replace(-1, 0)\n",
    "    \n",
    "    # Patient-based split\n",
    "    unique_patients = train_df['PatientID'].unique()\n",
    "    print(f\"Number of unique patients: {len(unique_patients)}\")\n",
    "    \n",
    "    gss = GroupShuffleSplit(n_splits=1, test_size=1-TRAIN_SPLIT, random_state=42)\n",
    "    train_idx, val_idx = next(gss.split(train_df, groups=train_df['PatientID']))\n",
    "    \n",
    "    train_data = train_df.iloc[train_idx]\n",
    "    val_data = train_df.iloc[val_idx]\n",
    "    \n",
    "    print(f\"\\nData split:\")\n",
    "    print(f\"Train: {len(train_data)} samples\")\n",
    "    print(f\"Val: {len(val_data)} samples\")\n",
    "    print(f\"Test: {len(test_df)} samples\")\n",
    "    \n",
    "    return train_data, val_data, test_df\n",
    "\n",
    "def create_transforms():\n",
    "    \"\"\"Create augmentation pipelines\"\"\"\n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "        A.CLAHE(clip_limit=2.0, p=0.3),\n",
    "        A.CoarseDropout(max_holes=8, max_height=20, max_width=20, p=0.2),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    return train_transform, val_transform\n",
    "\n",
    "def evaluate_model(predictions, true_labels, test_df):\n",
    "    \"\"\"Evaluate model performance - FIXED FOR BINARY LABELS\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\nPer-Disease Performance:\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'Disease':<30} {'AUC':>8} {'AP':>8} {'F1':>8} {'Sens':>8} {'Spec':>8}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for i, disease in enumerate(DISEASE_LABELS):\n",
    "        y_true = true_labels[:, i]\n",
    "        y_pred = predictions[disease]\n",
    "        \n",
    "        if y_true.sum() > 0 and y_true.sum() < len(y_true):\n",
    "            # Calculate metrics\n",
    "            auc = roc_auc_score(y_true, y_pred)\n",
    "            ap = average_precision_score(y_true, y_pred)\n",
    "            \n",
    "            # Find optimal threshold\n",
    "            precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "            f1_scores = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "            best_idx = np.argmax(f1_scores[:-1])\n",
    "            best_threshold = thresholds[best_idx]\n",
    "            \n",
    "            # Binary predictions\n",
    "            y_pred_binary = (y_pred > best_threshold).astype(int)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            tn = np.sum((y_true == 0) & (y_pred_binary == 0))\n",
    "            fp = np.sum((y_true == 0) & (y_pred_binary == 1))\n",
    "            fn = np.sum((y_true == 1) & (y_pred_binary == 0))\n",
    "            tp = np.sum((y_true == 1) & (y_pred_binary == 1))\n",
    "            \n",
    "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "            f1 = 2 * tp / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0\n",
    "            \n",
    "            results[disease] = {\n",
    "                'AUC': auc, 'AP': ap, 'F1': f1,\n",
    "                'Sensitivity': sensitivity, 'Specificity': specificity,\n",
    "                'Threshold': best_threshold\n",
    "            }\n",
    "            \n",
    "            all_metrics.append([disease, auc, ap, f1, sensitivity, specificity])\n",
    "            \n",
    "            print(f\"{disease:<30} {auc:>8.3f} {ap:>8.3f} {f1:>8.3f} \"\n",
    "                  f\"{sensitivity:>8.3f} {specificity:>8.3f}\")\n",
    "    \n",
    "    # Overall metrics\n",
    "    if all_metrics:\n",
    "        print(\"\\n\" + \"-\"*80)\n",
    "        print(\"Overall Performance Summary:\")\n",
    "        print(\"-\"*80)\n",
    "        \n",
    "        metrics_array = np.array([m[1:] for m in all_metrics])\n",
    "        metric_names = ['AUC', 'AP', 'F1', 'Sensitivity', 'Specificity']\n",
    "        \n",
    "        for i, name in enumerate(metric_names):\n",
    "            mean_val = np.mean(metrics_array[:, i])\n",
    "            std_val = np.std(metrics_array[:, i])\n",
    "            print(f\"Mean {name}: {mean_val:.3f} ± {std_val:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ================== RUN SYSTEM ==================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"MEMORY-SAFE ENSEMBLE MODEL SYSTEM\")\n",
    "    print(\"Fixed Label Handling with Binary Classification\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"- USE_ALL_TRAIN: {USE_ALL_TRAIN}\")\n",
    "    print(f\"- MEMORY_SAFE_MODE: {MEMORY_SAFE_MODE}\")\n",
    "    print(f\"- MODEL_TYPE: {MODEL_TYPE}\")\n",
    "    print(f\"- MAX_SAMPLES_PER_CHUNK: {MAX_SAMPLES_PER_CHUNK}\")\n",
    "    print(f\"- BATCH_SIZE: {BATCH_SIZE}\")\n",
    "    print(f\"- EPOCHS: {EPOCHS}\")\n",
    "    \n",
    "    # Run memory-safe pipeline\n",
    "    system, results = main_memory_safe()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Training completed successfully!\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-03T17:05:38.966006Z",
     "iopub.status.busy": "2025-08-03T17:05:38.965721Z",
     "iopub.status.idle": "2025-08-03T17:05:39.004123Z",
     "shell.execute_reply": "2025-08-03T17:05:39.003478Z",
     "shell.execute_reply.started": "2025-08-03T17:05:38.965985Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "G5aZAlVbn5yz"
   ],
   "include_colab_link": true,
   "name": "C1M2_Assignment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "coursera": {
   "schema_names": [
    "AI4MC1-1"
   ]
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 1302315,
     "sourceId": 2169393,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4064514,
     "sourceId": 7060263,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4069960,
     "sourceId": 7067871,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4217165,
     "sourceId": 7274254,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
